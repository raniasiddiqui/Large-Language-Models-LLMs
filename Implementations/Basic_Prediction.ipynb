{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whDeUBBC7Jim"
      },
      "source": [
        "## Notebook Objectives:\n",
        "\n",
        "In the last notebook, we covered **AutoTokenizer**. In this notebook, our focus will be on **AutoModel**.\n",
        "\n",
        "### Objectives:\n",
        "\n",
        "1. **Introduction to AutoModel**: We will explore the concept of AutoModel and understand its role in Hugging Face's Transformers library.\n",
        "\n",
        "2. **Providing Raw Data**: We will learn how to provide raw data to the AutoModel for inference. This involves preprocessing the input data, tokenizing it using AutoTokenizer, and feeding it to AutoModel for generating outputs.\n",
        "\n",
        "3. **Mapping Logit Scores**: We will learn how to interpret the output of AutoModel, which typically consists of logits. We will learn how to map these logits to positive and negative sentiment scores.\n",
        "\n",
        "By the end of this notebook, you will have a comprehensive understanding of how to utilize AutoModel for various NLP tasks, including sentiment analysis, by providing raw data and mapping the model outputs to meaningful predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epF6QV_R7Jiv"
      },
      "source": [
        "\n",
        "\n",
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbhtunAB7Jix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc44aa42-f679-40df-9a2d-66894895a4ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.17.1-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, responses, multiprocess, datasets, evaluate\n",
            "Successfully installed datasets-2.17.1 dill-0.3.8 evaluate-0.4.1 multiprocess-0.70.16 responses-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code utilizes the Hugging Face `transformers` library to create a sentiment analysis pipeline using a pre-trained model. Here's a breakdown of the code:\n",
        "\n",
        "1. **Importing the necessary modules**:\n",
        "   - The code begins by importing the `pipeline` function from the `transformers` library. This function allows us to easily create pipelines for various natural language processing (NLP) tasks.\n",
        "\n",
        "2. **Creating a sentiment analysis pipeline**:\n",
        "   - The `pipeline` function is used to create a sentiment analysis pipeline. This pipeline is capable of analyzing the sentiment (positive, negative, or neutral) of input text.\n",
        "   - The argument `\"sentiment-analysis\"` specifies that we want to create a pipeline specifically for sentiment analysis.\n",
        "\n",
        "3. **Performing sentiment analysis**:\n",
        "   - The `classifier` object created by the pipeline function is then used to analyze the sentiment of two input sentences.\n",
        "   - The input sentences are provided as a list within the `classifier` function call.\n",
        "   - The sentiment analysis pipeline analyzes each input sentence and predicts its sentiment.\n",
        "\n",
        "4. **Output**:\n",
        "   - The output of the `classifier` function call is a list of dictionaries, where each dictionary represents the sentiment analysis result for one of the input sentences.\n",
        "   - Each dictionary contains two keys:\n",
        "     - `\"label\"`: Represents the predicted sentiment label (e.g., \"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\").\n",
        "     - `\"score\"`: Represents the confidence score associated with the predicted sentiment label.\n",
        "\n"
      ],
      "metadata": {
        "id": "j1hZVXKxwhOH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "782xvHBM7Ji1",
        "outputId": "0cef7078-ab30-497f-92dd-9b286a0f28b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "ccadef829b2f4accabb5535654e4bc5f",
            "3b4f96496b7949a6b2ad9c1779045e03",
            "5f587732ccb240bb852707fa31724cce",
            "a8ab21488e8e4987802557bb90868a1c",
            "0045742df02f42bf86bfad77cddc288d",
            "b1a4f7d309c140e38d5acb2da5a53119",
            "8b9ceab82f6146759672dd1c7d19c907",
            "1b5117e549054fd99540c1621070034f",
            "286e9d1b83aa4b15a691142f4dde4560",
            "2d09685c3e6947db97e15bd16cb8ae03",
            "37c9cba8f9624c9aa2ad8bec7ce5b596"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ccadef829b2f4accabb5535654e4bc5f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598048329353333},\n",
              " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\n",
        "    [\n",
        "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "        \"I hate this so much!\",\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`classifier.model`: This line accesses the `model` attribute of the `classifier` object. The `model` attribute provides access to the underlying pre-trained model used by the `pipeline` object. You can use this attribute to directly interact with the model, access its parameters, fine-tune it on custom data, or perform any other operations supported by the underlying model architecture.\n"
      ],
      "metadata": {
        "id": "qJizhnRgw2y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# classifier details\n",
        "classifier.model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P24PKfnWuv3Z",
        "outputId": "f6014dc5-298f-41db-a57b-745348ebb2ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AutoTokenizer\n",
        "\n",
        "\n",
        "- `from transformers import AutoTokenizer`: This line imports the `AutoTokenizer` class from the Transformers library. Tokenizers are used to preprocess text data before feeding it into a model. The `AutoTokenizer` class automatically selects an appropriate tokenizer based on the provided model checkpoint name.\n",
        "\n",
        "- `checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"`: This line defines the checkpoint name of a pre-trained model for text classification. In this case, it specifies the \"distilbert-base-uncased-finetuned-sst-2-english\" checkpoint, which is a fine-tuned version of the DistilBERT model on the Stanford Sentiment Treebank (SST-2) dataset.\n",
        "\n",
        "- `tokenizer = AutoTokenizer.from_pretrained(checkpoint)`: This line initializes an instance of the `AutoTokenizer` class with the specified checkpoint name. The `from_pretrained` method loads the pre-trained tokenizer associated with the provided checkpoint. The tokenizer will be used to tokenize and preprocess text inputs for the text classification model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9ZXY60ylve5o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXDTdWlR7Ji6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- `inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")`: This line tokenizes the `raw_inputs` using a tokenizer. The `padding=True` argument ensures that sequences are padded to the maximum length, and `truncation=True` specifies that sequences longer than the maximum length should be truncated. The `return_tensors=\"pt\"` argument instructs the tokenizer to return PyTorch tensors.\n"
      ],
      "metadata": {
        "id": "99Dpq76cvi_1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Afg-cKYO7Ji7",
        "outputId": "b7a1ccab-9850-4dbe-c534-5d01b369bce7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------- input to model --------------------\n",
            "input to the model:  {'input_ids': tensor([[ 101, 1999, 2023, 5278, 5957, 1997, 9932, 1010, 1045, 2359, 2000, 4553,\n",
            "         2222, 5244, 1012,  102],\n",
            "        [ 101, 1045, 5223, 2784, 4083, 2061, 2172,  999,  102,    0,    0,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [ 101, 2393,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
            "-------- input mapping to ids --------------\n",
            "input ids tensor([[ 101, 1999, 2023, 5278, 5957, 1997, 9932, 1010, 1045, 2359, 2000, 4553,\n",
            "         2222, 5244, 1012,  102],\n",
            "        [ 101, 1045, 5223, 2784, 4083, 2061, 2172,  999,  102,    0,    0,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [ 101, 2393,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0]])\n",
            "-------- tokens to the model  --------------\n",
            "[CLS] in this changing landscape of ai, i wanted to learn llms. [SEP]\n",
            "[CLS] i hate deep learning so much! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "[CLS] help [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ]
        }
      ],
      "source": [
        "raw_inputs = [\n",
        "    \"In This changing landscape of AI, I wanted to learn LLMs.\",\n",
        "    \"I hate Deep Learning so much!\",\n",
        "    \"Help\"\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "#When you input 3 sentences of lengths (5, 10, 15) to the tokenizer with padding=True,\n",
        "#the tokenizer will pad each sentence to the same length,\n",
        "#which is determined by the longest sentence in the input.\n",
        "\n",
        "print(\"-------- input to model --------------------\")\n",
        "print(\"input to the model: \",inputs)\n",
        "print(\"-------- input mapping to ids --------------\")\n",
        "print(\"input ids\",inputs['input_ids'] )\n",
        "print(\"-------- tokens to the model  --------------\")\n",
        "\n",
        "tokenized_sentences = inputs[\"input_ids\"]\n",
        "\n",
        "# Loop through each tokenized sentence and print its tokens\n",
        "for sentence in tokenized_sentences:\n",
        "    # Decode the tokens to strings\n",
        "    sentence_tokens = tokenizer.decode(sentence)\n",
        "\n",
        "    # Print the tokens\n",
        "    print(sentence_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AutoModel\n",
        "- `model = AutoModel.from_pretrained(checkpoint)`: This initializes a new model object by loading a pre-trained model specified by the `checkpoint` parameter. The `AutoModel.from_pretrained()` method automatically downloads and loads the specified pre-trained model from the Hugging Face model hub.\n"
      ],
      "metadata": {
        "id": "hQoCeldwxwUz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObB55J3_7Ji8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModel.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Without unpacking\n",
        "outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
        "\n",
        "### With unpacking\n",
        "outputs = model(**inputs)"
      ],
      "metadata": {
        "id": "-zrQahhNP22S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6-UTBeP7Ji-",
        "outputId": "a24cd0da-2e13-4b7f-ca1d-8970da2a1b2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 16, 768])\n"
          ]
        }
      ],
      "source": [
        "#**inputs, it’s being used to unpack a dictionary into keyword arguments.\n",
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRYMcBqW7JjA"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCp0P9lQ7JjB",
        "outputId": "6684186a-9a69-4ba5-f4bd-3c1654a21cdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 2])\n"
          ]
        }
      ],
      "source": [
        "print(outputs.logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tm2eurwi7JjC",
        "outputId": "9535140d-2f4e-4f07-91b7-a889367460e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.5473,  1.5419],\n",
            "        [ 3.3221, -2.7394],\n",
            "        [-2.2521,  2.2720]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECZjKMl_7JjE",
        "outputId": "450d4139-eb46-46e2-c040-76e4aa334eb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0436, 0.9564],\n",
            "        [0.9977, 0.0023],\n",
            "        [0.0107, 0.9893]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFQL6VIY7JjF",
        "outputId": "8cba42d7-58dd-4ae1-f904-7dc8d488b307",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'NEGATIVE', 1: 'POSITIVE'}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "model.config.id2label"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map predictions to labels using argmax\n",
        "predicted_indices = torch.argmax(predictions, dim=-1)\n",
        "\n",
        "[model.config.id2label[idx.item()] for idx in predicted_indices]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrbC2CP2tGDu",
        "outputId": "1c2a6ab3-7b30-4db3-928e-de76368065c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['POSITIVE', 'NEGATIVE', 'POSITIVE']"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ixl504uQ7KBd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ccadef829b2f4accabb5535654e4bc5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b4f96496b7949a6b2ad9c1779045e03",
              "IPY_MODEL_5f587732ccb240bb852707fa31724cce",
              "IPY_MODEL_a8ab21488e8e4987802557bb90868a1c"
            ],
            "layout": "IPY_MODEL_0045742df02f42bf86bfad77cddc288d"
          }
        },
        "3b4f96496b7949a6b2ad9c1779045e03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1a4f7d309c140e38d5acb2da5a53119",
            "placeholder": "​",
            "style": "IPY_MODEL_8b9ceab82f6146759672dd1c7d19c907",
            "value": "model.safetensors: 100%"
          }
        },
        "5f587732ccb240bb852707fa31724cce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b5117e549054fd99540c1621070034f",
            "max": 267832558,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_286e9d1b83aa4b15a691142f4dde4560",
            "value": 267832558
          }
        },
        "a8ab21488e8e4987802557bb90868a1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d09685c3e6947db97e15bd16cb8ae03",
            "placeholder": "​",
            "style": "IPY_MODEL_37c9cba8f9624c9aa2ad8bec7ce5b596",
            "value": " 268M/268M [00:02&lt;00:00, 118MB/s]"
          }
        },
        "0045742df02f42bf86bfad77cddc288d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1a4f7d309c140e38d5acb2da5a53119": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b9ceab82f6146759672dd1c7d19c907": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b5117e549054fd99540c1621070034f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "286e9d1b83aa4b15a691142f4dde4560": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d09685c3e6947db97e15bd16cb8ae03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37c9cba8f9624c9aa2ad8bec7ce5b596": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}