{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ExNeMhT5poDc"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([\n",
        "    [1, 2, 3, 4],\n",
        "    [5, 6, 7, 8]\n",
        "])"
      ],
      "metadata": {
        "id": "zDcMtX2Bqfi6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Weight matrices for two attention heads (4x3)\n",
        "WQ_1 = np.array([\n",
        "    [0.1, 0.2, 0.3],\n",
        "    [0.4, 0.5, 0.6],\n",
        "    [0.7, 0.8, 0.9],\n",
        "    [1.0, 1.1, 1.2]\n",
        "])\n",
        "\n",
        "WK_1 = np.array([\n",
        "    [0.3, 0.2, 0.1],\n",
        "    [0.6, 0.5, 0.4],\n",
        "    [0.9, 0.8, 0.7],\n",
        "    [1.2, 1.1, 1.0]\n",
        "])\n",
        "\n",
        "WV_1 = np.array([\n",
        "    [0.7, 0.8, 0.9],\n",
        "    [0.4, 0.5, 0.6],\n",
        "    [0.1, 0.2, 0.3],\n",
        "    [1.3, 1.4, 1.5]\n",
        "])\n",
        "\n",
        "WQ_2 = np.array([\n",
        "    [0.2, 0.3, 0.4],\n",
        "    [0.5, 0.6, 0.7],\n",
        "    [0.8, 0.9, 1.0],\n",
        "    [1.1, 1.2, 1.3]\n",
        "])\n",
        "\n",
        "WK_2 = np.array([\n",
        "    [0.4, 0.3, 0.2],\n",
        "    [0.7, 0.6, 0.5],\n",
        "    [1.0, 0.9, 0.8],\n",
        "    [1.3, 1.2, 1.1]\n",
        "])\n",
        "\n",
        "WV_2 = np.array([\n",
        "    [0.9, 1.0, 1.1],\n",
        "    [0.6, 0.7, 0.8],\n",
        "    [0.3, 0.4, 0.5],\n",
        "    [1.4, 1.5, 1.6]\n",
        "])\n",
        "\n",
        "print(\"WQ_1:\\n\", WQ_1)\n",
        "print(\"WK_1:\\n\", WK_1)\n",
        "print(\"WV_1:\\n\", WV_1)\n",
        "print(\"WQ_2:\\n\", WQ_2)\n",
        "print(\"WK_2:\\n\", WK_2)\n",
        "print(\"WV_2:\\n\", WV_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZodOGFQEq-_6",
        "outputId": "1e8f18a1-d0ea-4efd-9319-63cd3422e1f3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WQ_1:\n",
            " [[0.1 0.2 0.3]\n",
            " [0.4 0.5 0.6]\n",
            " [0.7 0.8 0.9]\n",
            " [1.  1.1 1.2]]\n",
            "WK_1:\n",
            " [[0.3 0.2 0.1]\n",
            " [0.6 0.5 0.4]\n",
            " [0.9 0.8 0.7]\n",
            " [1.2 1.1 1. ]]\n",
            "WV_1:\n",
            " [[0.7 0.8 0.9]\n",
            " [0.4 0.5 0.6]\n",
            " [0.1 0.2 0.3]\n",
            " [1.3 1.4 1.5]]\n",
            "WQ_2:\n",
            " [[0.2 0.3 0.4]\n",
            " [0.5 0.6 0.7]\n",
            " [0.8 0.9 1. ]\n",
            " [1.1 1.2 1.3]]\n",
            "WK_2:\n",
            " [[0.4 0.3 0.2]\n",
            " [0.7 0.6 0.5]\n",
            " [1.  0.9 0.8]\n",
            " [1.3 1.2 1.1]]\n",
            "WV_2:\n",
            " [[0.9 1.  1.1]\n",
            " [0.6 0.7 0.8]\n",
            " [0.3 0.4 0.5]\n",
            " [1.4 1.5 1.6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part a"
      ],
      "metadata": {
        "id": "ZYtahqsF6IG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Q, K, V for Head 1\n",
        "Q_1 = np.dot(X, WQ_1)\n",
        "K_1 = np.dot(X, WK_1)\n",
        "V_1 = np.dot(X, WV_1)\n",
        "\n",
        "# Compute Q, K, V for Head 2\n",
        "Q_2 = np.dot(X, WQ_2)\n",
        "K_2 = np.dot(X, WK_2)\n",
        "V_2 = np.dot(X, WV_2)\n",
        "\n",
        "print(\"Head 1 - Q_1:\\n\", Q_1)\n",
        "print(\"Head 1 - K_1:\\n\", K_1)\n",
        "print(\"Head 1 - V_1:\\n\", V_1)\n",
        "\n",
        "print(\"Head 2 - Q_2:\\n\", Q_2)\n",
        "print(\"Head 2 - K_2:\\n\", K_2)\n",
        "print(\"Head 2 - V_2:\\n\", V_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq8-aB0drPXw",
        "outputId": "50d0e64b-2960-4d12-c25d-3967a399bf48"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head 1 - Q_1:\n",
            " [[ 7.   8.   9. ]\n",
            " [15.8 18.4 21. ]]\n",
            "Head 1 - K_1:\n",
            " [[ 9.   8.   7. ]\n",
            " [21.  18.4 15.8]]\n",
            "Head 1 - V_1:\n",
            " [[ 7.   8.   9. ]\n",
            " [17.  19.6 22.2]]\n",
            "Head 2 - Q_2:\n",
            " [[ 8.   9.  10. ]\n",
            " [18.4 21.  23.6]]\n",
            "Head 2 - K_2:\n",
            " [[10.   9.   8. ]\n",
            " [23.6 21.  18.4]]\n",
            "Head 2 - V_2:\n",
            " [[ 8.6  9.6 10.6]\n",
            " [21.4 24.  26.6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part b"
      ],
      "metadata": {
        "id": "sEpwf9fs6Kiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute attention scores\n",
        "def compute_attention_scores(Q, K):\n",
        "    dk = K.shape[-1]  # Get the dimension of the key matrix\n",
        "    return np.dot(Q, K.T) / np.sqrt(dk)\n",
        "\n",
        "# Compute attention scores for Head 1\n",
        "attention_scores_1 = compute_attention_scores(Q_1, K_1)\n",
        "\n",
        "# Compute attention scores for Head 2\n",
        "attention_scores_2 = compute_attention_scores(Q_2, K_2)\n",
        "\n",
        "print(\"Head 1 - Attention Scores:\\n\", attention_scores_1)\n",
        "print(\"Head 2 - Attention Scores:\\n\", attention_scores_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_4f8sKXrrbm",
        "outputId": "1e3e4cfb-43b1-411c-d5eb-4094d6ab2bb4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head 1 - Attention Scores:\n",
            " [[109.69655115 251.95565747]\n",
            " [251.95565747 578.59734577]]\n",
            "Head 2 - Attention Scores:\n",
            " [[139.14141487 324.35538123]\n",
            " [324.35538123 756.0286305 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to apply softmax along rows\n",
        "def apply_softmax(scores):\n",
        "     # This step is done for normalisation. You wont be expected to do normalisation in the exam.\n",
        "    exp_scores = np.exp(scores - np.max(scores,axis=1,keepdims=True))\n",
        "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "# Apply softmax to attention scores for Head 1\n",
        "softmax_attention_1 = apply_softmax(attention_scores_1)\n",
        "\n",
        "# Apply softmax to attention scores for Head 2\n",
        "softmax_attention_2 = apply_softmax(attention_scores_2)\n",
        "\n",
        "print(\"Head 1 - Softmax Attention Weights:\\n\", np.round(softmax_attention_1,3))\n",
        "print(\"Head 2 - Softmax Attention Weights:\\n\", np.round(softmax_attention_2,3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DH7MR006sNo1",
        "outputId": "2ec7280c-e496-4ca2-8828-c925eac688bb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head 1 - Softmax Attention Weights:\n",
            " [[0. 1.]\n",
            " [0. 1.]]\n",
            "Head 2 - Softmax Attention Weights:\n",
            " [[0. 1.]\n",
            " [0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute weighted sum (Z) for Head 1\n",
        "Z_1 = np.dot(softmax_attention_1, V_1)\n",
        "\n",
        "# Compute weighted sum (Z) for Head 2\n",
        "Z_2 = np.dot(softmax_attention_2, V_2)\n",
        "\n",
        "print(\"Head 1 - Weighted Sum (Z_1):\\n\", Z_1)\n",
        "print(\"Head 2 - Weighted Sum (Z_2):\\n\", Z_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHJr69UAuS0Y",
        "outputId": "b5608f5f-943b-4982-e395-48035a22a90c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head 1 - Weighted Sum (Z_1):\n",
            " [[17.  19.6 22.2]\n",
            " [17.  19.6 22.2]]\n",
            "Head 2 - Weighted Sum (Z_2):\n",
            " [[21.4 24.  26.6]\n",
            " [21.4 24.  26.6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part c"
      ],
      "metadata": {
        "id": "i9nFFjRB6Tpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate Z_1 and Z_2 along axis 1 (columns)\n",
        "Z = np.concatenate((Z_1, Z_2), axis=1)\n",
        "\n",
        "print(\"Multi-Headed Attention Output (Z):\\n\", Z)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS0ELyxvub1_",
        "outputId": "b124798c-63b8-42ca-c3ee-c84c95706b51"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-Headed Attention Output (Z):\n",
            " [[17.  19.6 22.2 21.4 24.  26.6]\n",
            " [17.  19.6 22.2 21.4 24.  26.6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zNHlOPlS6TAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Z.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UfykWesup7L",
        "outputId": "e748ac81-d877-4d18-9a69-49c8ab5dc3ae"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part d"
      ],
      "metadata": {
        "id": "LsVyBLD-6Wle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample values for the weight matrix W_0 (6x4)\n",
        "W_0 = np.array([\n",
        "    [0.1, 0.2, 0.3, 0.4],\n",
        "    [0.5, 0.6, 0.7, 0.8],\n",
        "    [0.9, 1.0, 1.1, 1.2],\n",
        "    [1.3, 1.4, 1.5, 1.6],\n",
        "    [1.7, 1.8, 1.9, 2.0],\n",
        "    [2.1, 2.2, 2.3, 2.4]\n",
        "])\n"
      ],
      "metadata": {
        "id": "yw5St45UvTsj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the weight matrix W_0 with shape 6x4 (sample values)\n",
        "# Random values for demonstration, replace with actual matrix W_0\n",
        "\n",
        "# Multiply the concatenated multi-headed attention output Z by the weight matrix W_0\n",
        "result = np.dot(Z, W_0)  # Transpose W_0 to match dimensions for multiplication\n",
        "\n",
        "print(\"Result after multiplication with W_0:\\n\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9dL-6jJu_Cs",
        "outputId": "06d51c7a-260f-487b-9e7c-90df87ea867e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result after multiplication with W_0:\n",
            " [[155.96 169.04 182.12 195.2 ]\n",
            " [155.96 169.04 182.12 195.2 ]]\n"
          ]
        }
      ]
    }
  ]
}